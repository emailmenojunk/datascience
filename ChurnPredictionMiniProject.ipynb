{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPsPuIH5IWmzFab+3FVAZ9M",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/emailmenojunk/datascience/blob/main/ChurnPredictionMiniProject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "mr75k-CCNDQs",
        "outputId": "b7073826-da5c-441a-9e56-e32facf5b87d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.12/dist-packages (1.40.69)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: s3fs in /usr/local/lib/python3.12/dist-packages (0.4.2)\n",
            "Requirement already satisfied: botocore<1.41.0,>=1.40.69 in /usr/local/lib/python3.12/dist-packages (from boto3) (1.40.69)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from boto3) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.15.0,>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from boto3) (0.14.0)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: fsspec>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from s3fs) (2025.3.0)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.12/dist-packages (from botocore<1.41.0,>=1.40.69->boto3) (2.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "# Install necessary libraries like boto to take care of AWS S3 access\n",
        "!pip install boto3 pandas s3fs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import boto3\n",
        "from io import StringIO\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "# Retrieve AWS credentials securely from Colab Secrets\n",
        "# These are needed to access the S3 bucket.\n",
        "try:\n",
        "    os.environ['AWS_ACCESS_KEY_ID'] = userdata.get('accesskey')\n",
        "    os.environ['AWS_SECRET_ACCESS_KEY'] = userdata.get('secret')\n",
        "except KeyError:\n",
        "    print(\"ERROR: AWS credentials issue. Please check the Secrects in COLAB\")\n",
        "    raise # Stop execution if there is a credential issue\n",
        "\n",
        "# --- AWS S3 Bucket Details & Region  ---\n",
        "S3_BUCKET_NAME = 'amzn-s3-sagemaker-demo-bucket'\n",
        "# The specific folder and file path within the S3 bucket\n",
        "S3_KEY = 'SageMakerDemoFolder/churndata.csv'\n",
        "AWS_REGION = 'us-east-1'\n",
        "\n",
        "# Initialize the S3 client to interact with AWS S3\n",
        "s3 = boto3.client('s3', region_name=AWS_REGION)\n",
        "\n",
        "# Access the S3 object (the CSV file) and read its content directly into a pandas DataFrame\n",
        "try:\n",
        "    print(f\"Attempting to fetch s3://{S3_BUCKET_NAME}/{S3_KEY}...\")\n",
        "    obj = s3.get_object(Bucket=S3_BUCKET_NAME, Key=S3_KEY)\n",
        "\n",
        "    # Read the data from the S3 object's body and decode it from bytes to a string\n",
        "    data_body = obj['Body'].read().decode('utf-8')\n",
        "    # Use StringIO to treat the string data as a file for pandas to read\n",
        "    data = pd.read_csv(StringIO(data_body))\n",
        "\n",
        "    print(\"\\n Data successfully loaded from S3.\")\n",
        "    print(f\"DF shape: {data.shape}\")\n",
        "    print(\"\\nInitial 5 rows of the data:\")\n",
        "    print(data.head())\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n An error during S3 data loading: {e}\")\n",
        "    print(f\"Is the S3 Key correct ? : '{S3_KEY}'\")\n",
        "    print(\"Veify AWS permission & region\")\n",
        "\n",
        "# --- Target Variable Conversion ---\n",
        "# Convert the 'retained' column into a numerical 'target' variable (1 for Retained, 0 for Churn)\n",
        "# This is done here as part of the initial data loading and basic cleaning.\n",
        "ACTUAL_TARGET_COL_NAME = 'retained'\n",
        "data.loc[:, 'target'] = data[ACTUAL_TARGET_COL_NAME].astype(int)\n",
        "print(\"\\n--- Target Variable Distribution ---\")\n",
        "print(data['target'].value_counts())\n",
        "# Drop the original 'retained' column as it's no longer needed\n",
        "data.drop(ACTUAL_TARGET_COL_NAME, axis=1, inplace=True)\n",
        "print(\"\\nOriginal 'retained' column dropped, 'target' column created.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U4sRga4zPU5_",
        "outputId": "ec487771-cc25-4095-cb63-14b2a4ffd54b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attempting to fetch s3://amzn-s3-sagemaker-demo-bucket/SageMakerDemoFolder/churndata.csv...\n",
            "\n",
            " Data successfully loaded from S3.\n",
            "DF shape: (30801, 15)\n",
            "\n",
            "Initial 5 rows of the data:\n",
            "   custid  retained   created firstorder lastorder  esent   eopenrate  \\\n",
            "0  6H6T6N         0   9/28/12    8/11/13   8/11/13     29  100.000000   \n",
            "1  APCENR         1  12/19/10     4/1/11   1/19/14     95   92.631579   \n",
            "2  7UP6MS         0   10/3/10    12/1/10    7/6/11      0    0.000000   \n",
            "3  7ZEW8G         0  10/22/10    3/28/11   3/28/11      0    0.000000   \n",
            "4  8V726M         1  11/27/10   11/29/10   1/28/13     30   90.000000   \n",
            "\n",
            "   eclickrate  avgorder   ordfreq  paperless  refill  doorstep     favday city  \n",
            "0    3.448276     14.52  0.000000          0       0         0     Monday  DEL  \n",
            "1   10.526316     83.69  0.181641          1       1         1     Friday  DEL  \n",
            "2    0.000000     33.58  0.059908          0       0         0  Wednesday  DEL  \n",
            "3    0.000000     54.96  0.000000          0       0         0   Thursday  BOM  \n",
            "4   13.333333    111.91  0.008850          0       0         0     Monday  BOM  \n",
            "\n",
            "--- Target Variable Distribution ---\n",
            "target\n",
            "1    24472\n",
            "0     6329\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Original 'retained' column dropped, 'target' column created.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from io import StringIO\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "# This step prevents data corruption from previous cells.\n",
        "try:\n",
        "    obj = s3.get_object(Bucket=S3_BUCKET_NAME, Key=S3_KEY)\n",
        "    data = pd.read_csv(StringIO(obj['Body'].read().decode('utf-8')))\n",
        "    print(\"Data successfully reloaded and ready for feature engineering\")\n",
        "except NameError:\n",
        "    print(\"Error - Ensure Cell 1 (S3 Load) has been run to define 's3' and S3 details.\")\n",
        "    raise # Stop execution if there is a credential issue\n",
        "\n",
        "# Date format mm/dd/yy (Standardized to resolve original error)\n",
        "DATE_FORMAT = '%m/%d/%y'\n",
        "\n",
        "#  Date Conversion (Using .loc )\n",
        "data.loc[:, 'created'] = pd.to_datetime(data['created'], format=DATE_FORMAT, errors='coerce')\n",
        "data.loc[:, 'firstorder'] = pd.to_datetime(data['firstorder'], format=DATE_FORMAT, errors='coerce')\n",
        "data.loc[:, 'lastorder'] = pd.to_datetime(data['lastorder'], format=DATE_FORMAT, errors='coerce')\n",
        "\n",
        "# Target Conversion Logic\n",
        "ACTUAL_TARGET_COL_NAME = 'retained'\n",
        "# 1. Create the 'target' column based on 'retained' and convert to integer\n",
        "data.loc[:, 'target'] = data[ACTUAL_TARGET_COL_NAME].astype(int)\n",
        "print(\"\\nTarget Variable Distribution\")\n",
        "print(data['target'].value_counts())\n",
        "# 2. Drop the original 'retained' column\n",
        "data.drop(ACTUAL_TARGET_COL_NAME, axis=1, inplace=True)\n",
        "\n",
        "\n",
        "#  Initial Missing Value Handling (Using .loc )\n",
        "data.loc[:, 'esent'] = data['esent'].fillna(0)\n",
        "data.loc[:, 'eopenrate'] = data['eopenrate'].fillna(0)\n",
        "data.loc[:, 'eclickrate'] = data['eclickrate'].fillna(0)\n",
        "data.loc[:, 'ordfreq'] = data['ordfreq'].fillna(0)\n",
        "\n",
        "median_avgorder = data['avgorder'].median()\n",
        "data.loc[:, 'avgorder'] = data['avgorder'].fillna(median_avgorder)\n",
        "\n",
        "\n",
        "#  Feature Engineering\n",
        "\n",
        "# FEATURE 1: Customer Tenure (Account Age in Days)\n",
        "time_delta_tenure = (datetime.today() - data['created']).astype('timedelta64[ns]')\n",
        "data.loc[:, 'Customer_Tenure'] = time_delta_tenure.dt.days\n",
        "median_tenure = data['Customer_Tenure'].median()\n",
        "data.loc[:, 'Customer_Tenure'] = data['Customer_Tenure'].fillna(median_tenure)\n",
        "\n",
        "\n",
        "# FEATURE 2: Recency (Days Since Last Order)\n",
        "recency_delta = (datetime.today() - data['lastorder']).astype('timedelta64[ns]')\n",
        "data.loc[:, 'Recency_Days'] = recency_delta.dt.days\n",
        "data.loc[:, 'Recency_Days'] = data['Recency_Days'].fillna(data['Customer_Tenure'])\n",
        "median_recency = data['Recency_Days'].median()\n",
        "data.loc[:, 'Recency_Days'] = data['Recency_Days'].fillna(median_recency)\n",
        "\n",
        "\n",
        "# FEATURE 3: Email Engagement Score (Composite Feature)\n",
        "data.loc[:, 'Email_Engagement_Score'] = (data['eopenrate'] + data['eclickrate']) * data['esent']\n",
        "median_ees = data['Email_Engagement_Score'].median()\n",
        "data.loc[:, 'Email_Engagement_Score'] = data['Email_Engagement_Score'].fillna(median_ees)\n",
        "\n",
        "\n",
        "# FEATURE 4: Normalized Order Frequency\n",
        "data.loc[:, 'Customer_Tenure_Adj'] = data['Customer_Tenure'].apply(lambda x: x if x > 0 else 1)\n",
        "data.loc[:, 'Avg_Daily_Order_Freq'] = data['ordfreq'] / data['Customer_Tenure_Adj']\n",
        "median_avg_freq = data['Avg_Daily_Order_Freq'].median()\n",
        "data.loc[:, 'Avg_Daily_Order_Freq'] = data['Avg_Daily_Order_Freq'].fillna(median_avg_freq)\n",
        "\n",
        "\n",
        "# Clean up / drop unnecessary columns\n",
        "data.drop(['custid', 'created', 'firstorder', 'lastorder', 'Customer_Tenure_Adj'], axis=1, inplace=True)\n",
        "\n",
        "print(\"\\n Feature Engineering and Data Cleaning Complete.\")\n",
        "print(f\"Final DataFrame Shape: {data.shape}\")\n",
        "\n",
        "# Define Features and Target\n",
        "X = data.drop('target', axis=1)\n",
        "y = data['target']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X,\n",
        "    y,\n",
        "    test_size=0.3, random_state=123, stratify=y\n",
        ")\n",
        "print(f\"y_test class counts after re-split: \\n{y_test.value_counts()}\")\n",
        "\n",
        "# Define the preprocessing steps\n",
        "numeric_features = X.select_dtypes(include=np.number).columns\n",
        "categorical_features = X.select_dtypes(include='object').columns\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), numeric_features),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
        "    ],\n",
        "    remainder='passthrough'\n",
        ")\n",
        "print(\"\\nData splitting and preprocessing pipelines defined.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MNqCid7k-TDa",
        "outputId": "83fad8d4-168a-48bc-8898-141d4695657b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data successfully reloaded and ready for feature engineering\n",
            "\n",
            "Target Variable Distribution\n",
            "target\n",
            "1    24472\n",
            "0     6329\n",
            "Name: count, dtype: int64\n",
            "\n",
            " Feature Engineering and Data Cleaning Complete.\n",
            "Final DataFrame Shape: (30801, 15)\n",
            "y_test class counts after re-split: \n",
            "target\n",
            "1    7342\n",
            "0    1899\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Data splitting and preprocessing pipelines defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n Vaidate the First 10 Rows of Engineered Features \")\n",
        "\n",
        "# Display the first 10 rows of the complete feature set (X)\n",
        "# Display Days columns\n",
        "print(X.head(10)[['Customer_Tenure', 'Recency_Days', 'Email_Engagement_Score', 'Avg_Daily_Order_Freq', 'city']].to_string())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7OxrH6F8_iP",
        "outputId": "5f7ab536-1ba5-4e70-93dc-7d65b257283f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Vaidate the First 10 Rows of Engineered Features \n",
            "   Customer_Tenure  Recency_Days  Email_Engagement_Score  Avg_Daily_Order_Freq city\n",
            "0           4789.0        4472.0                  3000.0              0.000000  DEL\n",
            "1           5438.0        4311.0                  9800.0              0.000033  DEL\n",
            "2           5515.0        5239.0                     0.0              0.000011  DEL\n",
            "3           5496.0        5339.0                     0.0              0.000000  BOM\n",
            "4           5460.0        4667.0                  3100.0              0.000002  BOM\n",
            "5           6200.0        4316.0                  4400.0              0.000023  DEL\n",
            "6           5402.0        4314.0                  3000.0              0.000023  BOM\n",
            "7           5818.0        4314.0                  2800.0              0.000007  DEL\n",
            "8           5530.0        5478.0                     0.0              0.000000  DEL\n",
            "9           5428.0        4351.0                  3500.0              0.000025  DEL\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "# Define Random Forest Classifier Model\n",
        "# Define Pipeline to combine preprocessing and the classifier\n",
        "rf_model = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor), # Apply the preprocessing steps\n",
        "    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')) # Define the Random Forest classifier\n",
        "])\n",
        "\n",
        "\n",
        "print(\"\\n...Starting RF Model Training...\")\n",
        "# Train the model using the training data\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "print(\"Training of the model completed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T_9xOdsbaC9_",
        "outputId": "cfe30e3b-39a1-4603-fdba-ee7de31b2050"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "...Starting RF Model Training...\n",
            "Training of the model completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import f1_score, make_scorer\n",
        "\n",
        "# 1. Define the F1 scorer\n",
        "f1_scorer = make_scorer(f1_score, pos_label=1)\n",
        "\n",
        "print(\"\\n Model Evaluation - 5Fold CV - F1 Score \")\n",
        "\n",
        "# 2. Perform Cross-Validation on the ENTIRE dataset (X, y)\n",
        "f1_scores = cross_val_score(\n",
        "    rf_model, # pipeline\n",
        "    X, #  feature set\n",
        "    y, # target set\n",
        "    cv=5, #  5 folds\n",
        "    scoring=f1_scorer,\n",
        "    n_jobs=-1 #  all cores\n",
        ")\n",
        "\n",
        "# 3. Calculate the average and standard deviation of the scores\n",
        "mean_f1 = f1_scores.mean()\n",
        "std_f1 = f1_scores.std()\n",
        "\n",
        "print(f\"\\nIndividual Fold F1 Scores: {f1_scores}\")\n",
        "print(f\"\\nAverage F1 Score (5-Fold CV): **{mean_f1:.4f}**\")\n",
        "print(f\"Standard Deviation: (+/- {std_f1:.4f})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P7ZLkC2baI7M",
        "outputId": "21d05476-24ca-44a1-9ee8-2d5391d815c7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Model Evaluation - 5Fold CV - F1 Score \n",
            "\n",
            "Individual Fold F1 Scores: [0.98536388 0.99406224 0.89931401 0.90630256 0.95333658]\n",
            "\n",
            "Average F1 Score (5-Fold CV): **0.9477**\n",
            "Standard Deviation: (+/- 0.0391)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a93b6292",
        "outputId": "00ca891e-3e1d-4caf-d36f-35229b474ae7"
      },
      "source": [
        "import joblib\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Define the filename for the model\n",
        "model_filename = 'churn_prediction_rf_model.joblib'\n",
        "\n",
        "# Save the trained model to the file\n",
        "try:\n",
        "    joblib.dump(rf_model, model_filename)\n",
        "    print(f\"\\n Trained model saved successfully as {model_filename}\")\n",
        "except NameError:\n",
        "    print(\"\\nError - 'rf_model' not found. Please ensure the model training cell was run.\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n An error occurred while saving the model: {e}\")\n",
        "# Load the trained model from the file\n",
        "model_filename = 'churn_prediction_rf_model.joblib'\n",
        "try:\n",
        "    loaded_model = joblib.load(model_filename)\n",
        "    print(f\"\\n Model loaded from {model_filename}\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"\\n Error: Model file '{model_filename}' not found. Please run previous cells to train and save the model.\")\n",
        "    loaded_model = None # Set to None to avoid errors later\n",
        "\n",
        "\n",
        "if loaded_model is not None:\n",
        "    # Get sample data for row 1 from the test set (iloc for row index)\n",
        "    if len(X_test) > 1:\n",
        "        sample_data_row_1 = X_test.iloc[[1]]\n",
        "        original_index_row_1 = X_test.index[1] # Get the original index label\n",
        "        print(f\"\\n Successfully selected sample data for row 1 (original index {original_index_row_1}).\")\n",
        "\n",
        "        # Make a prediction using the loaded pipeline on the sample data\n",
        "        test_prediction_row_1 = loaded_model.predict(sample_data_row_1)[0]\n",
        "\n",
        "        # Calculate Predicted Probability\n",
        "        proba_output_row_1 = loaded_model.predict_proba(sample_data_row_1)\n",
        "\n",
        "        # Handle the case where predict_proba returns only one column\n",
        "        if proba_output_row_1.shape[1] == 1:\n",
        "            # If only one column, it's typically the probability of the negative class (Churn)\n",
        "            test_proba_churn_row_1 = proba_output_row_1[0, 0]\n",
        "            test_proba_retained_row_1 = 1 - test_proba_churn_row_1 # Probability of Retained is 1 - Churn probability\n",
        "            print(\"  Note: predict_proba returned 1 column. Assuming it's Churn probability (Index 0).\")\n",
        "        else:\n",
        "            # Normal case: two columns are returned. Churn is index 0, Retained is index 1.\n",
        "            test_proba_churn_row_1 = proba_output_row_1[0, 0]\n",
        "            test_proba_retained_row_1 = proba_output_row_1[0, 1]\n",
        "            print(\"  Note: predict_proba returned 2 columns. Using Index 0 for Churn, Index 1 for Retained.\")\n",
        "\n",
        "\n",
        "        print(\"\\n--- Deployment Test for Sample at Row 1 of Test Data ---\")\n",
        "        print(f\"Sample Input (Original Index {original_index_row_1}):\\n{sample_data_row_1.to_string(index=False)}\")\n",
        "        print(f\"Predicted Status (0=Churn, 1=Retained): {test_prediction_row_1}\")\n",
        "        print(f\"Prediction Probability of CHURN: {test_proba_churn_row_1:.4f}\")\n",
        "        print(f\"Prediction Probability of RETAINED: **{test_proba_retained_row_1:.4f}**\")\n",
        "\n",
        "    else:\n",
        "        print(\"\\n Test set does not have a row at index 1 (it might be empty or too small).\")\n",
        "\n",
        "else:\n",
        "    print(\"\\nCannot perform deployment test as model was not loaded.\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Trained model saved successfully as churn_prediction_rf_model.joblib\n",
            "\n",
            " Model loaded from churn_prediction_rf_model.joblib\n",
            "\n",
            " Successfully selected sample data for row 1 (original index 18898).\n",
            "  Note: predict_proba returned 2 columns. Using Index 0 for Churn, Index 1 for Retained.\n",
            "\n",
            "--- Deployment Test for Sample at Row 1 of Test Data ---\n",
            "Sample Input (Original Index 18898):\n",
            " esent  eopenrate  eclickrate  avgorder  ordfreq  paperless  refill  doorstep   favday city  Customer_Tenure  Recency_Days  Email_Engagement_Score  Avg_Daily_Order_Freq\n",
            "    31   6.451613    3.225806     40.02      0.0          1       1         0 Thursday  BOM           4395.0        4395.0                   300.0                   0.0\n",
            "Predicted Status (0=Churn, 1=Retained): 1\n",
            "Prediction Probability of CHURN: 0.0000\n",
            "Prediction Probability of RETAINED: **1.0000**\n"
          ]
        }
      ]
    }
  ]
}